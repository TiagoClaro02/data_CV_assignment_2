{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eih0xp3OKoqX"
   },
   "source": [
    "# CV - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LP74kSOSjV1"
   },
   "source": [
    "Alexandre Vilhena da Costa\n",
    "\n",
    "Tiago Marques Claro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFq9zpm7SZG3"
   },
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJUwfQN-Sg07"
   },
   "source": [
    "This work aims to develop a model capable of detecting sacrificial anodes in underwater environments. The dataset used in this assignment was captured in two distinct settings: an indoor pool with minimal noise and clear images (figure 1), and the sea, where the presence of biomatter and suspended particles poses a significant challenge for detecting anodes (figure). Additionally, as depth increases, the absorption of light begins to filter out certain wavelengths, resulting in images dominated by green and blue hues [1].\n",
    "\n",
    "To address these challenges, this work begins by pre-processing the dataset to enhance image quality and splitting it into training, validation, and testing subsets. Subsequently, we design and implement Neural Networks, which are trained, validated, and tested using this dataset. Finally, we present a discussion of the results, including a comparison with state-of-the-art models.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"266.png\" alt=\"Underwater Image\" width=\"400\"/>\n",
    "  <p><em>Figure 1: Pool Image</em></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"5482.png\" alt=\"Underwater Image\" width=\"400\"/>\n",
    "  <p><em>Figure 2: Underwater Image</em></p>\n",
    "</div>\n",
    "\n",
    "[1] Zhou, J., Yang, T. & Zhang, W. Underwater vision enhancement technologies: a comprehensive review, challenges, and recent trends. Appl Intell 53, 3594–3621 (2023). https://doi.org/10.1007/s10489-022-03767-y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYptV76qNX8L"
   },
   "source": [
    "## **Dataset Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1B3dsjoNw4z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIInp4YlNwLZ"
   },
   "source": [
    "### Definition of the paths to the organized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we created our organized dataset in the following folder structure:\n",
    "\n",
    "organized_dataset/           \n",
    "├── images/                     \n",
    "│  \t ├── train/                   \n",
    "│    ├── val/                    \n",
    "│\t └── test/                 \n",
    "├── labels/                     \n",
    "│  \t ├── train/                   \n",
    "│\t ├── val/                     \n",
    "│  \t └── test/                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcUWbAvFN7xs"
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "datasets_root = \"datasets\"  # Root folder containing the subfolders\n",
    "output_dir = \"organized_dataset\"  # Root directory for the organized dataset\n",
    "images_dir = os.path.join(output_dir, \"images\")\n",
    "labels_dir = os.path.join(output_dir, \"labels\")\n",
    "train_dir = os.path.join(images_dir, \"train\")\n",
    "val_dir = os.path.join(images_dir, \"val\")\n",
    "test_dir = os.path.join(images_dir, \"test\")\n",
    "train_labels_dir = os.path.join(labels_dir, \"train\")\n",
    "val_labels_dir = os.path.join(labels_dir, \"val\")\n",
    "test_labels_dir = os.path.join(labels_dir, \"test\")\n",
    "\n",
    "# Create output directories for images and labels\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(train_labels_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "os.makedirs(test_labels_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLj76vrCOAEt"
   },
   "source": [
    "### Split the images and labels into the training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMDPWLxYPlhj"
   },
   "outputs": [],
   "source": [
    "# Collect all images and labels\n",
    "image_label_pairs = []\n",
    "for folder in os.listdir(datasets_root):\n",
    "    folder_path = os.path.join(datasets_root, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        images_path = os.path.join(folder_path, \"images\")\n",
    "        labels_path = os.path.join(folder_path, \"labels\")\n",
    "\n",
    "        if os.path.exists(images_path) and os.path.exists(labels_path):\n",
    "            images = os.listdir(images_path)\n",
    "            for image in images:\n",
    "                label = image.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")  # Adjust for label naming\n",
    "                image_path = os.path.join(images_path, image)\n",
    "                label_path = os.path.join(labels_path, label)\n",
    "\n",
    "                if os.path.exists(image_path) and os.path.exists(label_path):\n",
    "                    image_label_pairs.append((image_path, label_path))\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(image_label_pairs)\n",
    "\n",
    "# Split into train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_size = len(image_label_pairs)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "\n",
    "train_set = image_label_pairs[:train_size]\n",
    "val_set = image_label_pairs[train_size:train_size + val_size]\n",
    "test_set = image_label_pairs[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-WNFMi3QDjJ"
   },
   "source": [
    "### Application of CLAHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underwater images often lose red light frequencies at depths of less than 10 meters, resulting in a predominantly bluish-green appearance. To address this, we applied the CLAHE (Contrast Limited Adaptive Histogram Equalization) technique. This method enhances the image histogram, improving contrast and making it easier to identify the anodes. The following image illustrates this approach.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"histogram_equalization.png\" alt=\"Underwater Image\" width=\"400\"/>\n",
    "  <p><em>Figure 3: Contrast Limited Adaptive Histogram Equalization</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul0DFYuSQW7h"
   },
   "outputs": [],
   "source": [
    "# Function to apply CLAHE processing to an image\n",
    "def apply_clahe(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to the L-channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "\n",
    "    # Merge channels and convert back to BGR\n",
    "    lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "    processed_image = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below shows the results on three different samples, where the contrast has been enhanced. This improvement allows the CNN to more effectively detect the anodes.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"Pasted image.png\" alt=\"Underwater Image\" width=\"800\"/>\n",
    "  <p><em>Figure 4: Application of CLAHE</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxuSR7txQY1c"
   },
   "source": [
    "### Save the images of each dataset into the respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVFtLsFANrW4"
   },
   "outputs": [],
   "source": [
    "def copy_and_process_files(file_pairs, image_dest, label_dest, prefix):\n",
    "    for i, (image_path, label_path) in enumerate(tqdm(file_pairs, desc=f\"Processing {prefix} set\")):\n",
    "        unique_id = f\"{prefix}_{i}\"\n",
    "        new_image_name = f\"{unique_id}.jpg\"\n",
    "        new_label_name = f\"{unique_id}.txt\"\n",
    "\n",
    "        # Apply CLAHE processing\n",
    "        processed_image = apply_clahe(image_path)\n",
    "\n",
    "        # Save the processed image\n",
    "        processed_image_path = os.path.join(image_dest, new_image_name)\n",
    "        cv2.imwrite(processed_image_path, processed_image)\n",
    "\n",
    "        # Copy the label file\n",
    "        shutil.copy(label_path, os.path.join(label_dest, new_label_name))\n",
    "\n",
    "# Copy and process train, validation, and test sets\n",
    "copy_and_process_files(train_set, train_dir, train_labels_dir, \"train\")\n",
    "copy_and_process_files(val_set, val_dir, val_labels_dir, \"val\")\n",
    "copy_and_process_files(test_set, test_dir, test_labels_dir, \"test\")\n",
    "\n",
    "print(f\"Dataset organized and processed successfully!\")\n",
    "print(f\"Train set: {len(train_set)} images\")\n",
    "print(f\"Validation set: {len(val_set)} images\")\n",
    "print(f\"Test set: {len(test_set)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igiWwjtONcZx"
   },
   "source": [
    "## **CNN implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY23xv7lLi8H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_hySlBJLXqN"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYIp78YYdF66"
   },
   "source": [
    "To train and test our model, we first need to create a custom class for our dataset that converts the provided information into an image and its corresponding bounding box. For this purpose, we developed the AnodeDataset class, which takes the following parameters: the images directory, the annotations directory, the image size, and the transformations to be applied.\n",
    "\n",
    "The image size parameter is used to resize the input images, enabling a more efficient training process. The transform parameter defines the data augmentation procedures, like changes in contrast, saturation, etc... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teDLoU2GKiEa"
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class AnodeDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, img_size, transform=None):\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # Match images and annotations by filenames\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith((\".jpg\", \".png\"))])\n",
    "        self.annotation_files = sorted([f for f in os.listdir(annotation_dir) if f.endswith(\".txt\")])\n",
    "        assert len(self.image_files) == len(self.annotation_files), \"Mismatch in images and annotations count.\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "\t\t# Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        # Resize image\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "\n",
    "        # Load annotation\n",
    "        anno_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "\n",
    "\t\t# If annotation file is empty, return an empty target\n",
    "        if os.path.exists(anno_path) and os.path.getsize(anno_path) > 0:\n",
    "            with open(anno_path, \"r\") as f:\n",
    "                line = f.readline().strip()\n",
    "            if line:  # File is not empty and has valid content\n",
    "                _, x_center, y_center, width, height = map(float, line.split())\n",
    "                bboxes = [[x_center, y_center, width, height]]\n",
    "            else:\n",
    "                bboxes = []\n",
    "        else:\n",
    "            bboxes = []\n",
    "\n",
    "\t\t# Apply augmentations\n",
    "        if self.transform and bboxes:\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "\n",
    "\t\t# Normalize image and convert to tensor\n",
    "        img = img / 255.0\n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # If there are no bounding boxes, return a dummy target\n",
    "        if not bboxes:\n",
    "            target = torch.tensor([0, 0, 0, 0], dtype=torch.float32)\n",
    "        else:\n",
    "            target = torch.tensor(bboxes[0], dtype=torch.float32)  # Assuming one bounding box per image\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmqWqoTRLucd"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_wlhjVCL0vy"
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, img_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        feature_size = 256 * (img_size // 32) * (img_size // 32)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),    # Dropout layer to prevent overfitting\n",
    "            nn.Linear(128, 4),  # 4 for bbox\n",
    "            nn.Sigmoid()        # Sigmoid activation for values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr1XnxS6MMoO"
   },
   "source": [
    "### Loss Function and IoU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q43_PAZuMRxM"
   },
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def loss_function(predictions, targets):\n",
    "    pred_boxes = predictions[:, :4]  # x_center, y_center, width, height\n",
    "    target_boxes = targets[:, :4]\n",
    "\n",
    "    bbox_loss = nn.L1Loss()(pred_boxes, target_boxes)\n",
    "\n",
    "    return bbox_loss\n",
    "\n",
    "# Function to calculate Intersection over Union (IoU)\n",
    "def calculate_iou(pred_box, gt_box):\n",
    "    x1, y1, x2, y2 = pred_box\n",
    "    gx1, gy1, gx2, gy2 = gt_box\n",
    "\n",
    "    # Compute intersection area\n",
    "    inter_x1 = max(x1, gx1)\n",
    "    inter_y1 = max(y1, gy1)\n",
    "    inter_x2 = min(x2, gx2)\n",
    "    inter_y2 = min(y2, gy2)\n",
    "\n",
    "    inter_width = max(0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0, inter_y2 - inter_y1)\n",
    "    intersection_area = inter_width * inter_height\n",
    "\n",
    "    # Compute areas\n",
    "    pred_area = (x2 - x1) * (y2 - y1)\n",
    "    gt_area = (gx2 - gx1) * (gy2 - gy1)\n",
    "\n",
    "    # Compute IoU\n",
    "    union_area = pred_area + gt_area - intersection_area\n",
    "    iou = intersection_area / union_area if union_area > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67n44L1kMYcx"
   },
   "source": [
    "### Model Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu_zCkbGMbg_"
   },
   "outputs": [],
   "source": [
    "def save_model(model, num_epochs):\n",
    "\n",
    "    model_name = f\"anode_detector_{num_epochs}.pth\"\n",
    "    os.makedirs(f\"models/{num_epochs}\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"models/{num_epochs}/{model_name}\")\n",
    "\n",
    "\t# Save the model neural network architecture in a txt file\n",
    "    with open(f\"models/{num_epochs}/model_architecture.txt\", \"w\") as f:\n",
    "\t    f.write(str(model))\n",
    "\n",
    "def makedirs_clean(name, mode=0o777, exist_ok=True):\n",
    "    if os.path.exists(name):\n",
    "        if exist_ok:\n",
    "            shutil.rmtree(name)  # Remove the existing directory and its contents\n",
    "        else:\n",
    "            raise OSError(f\"Directory {name} already exists.\")\n",
    "    os.makedirs(name, mode=mode, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2RDKDTjMj8L"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHUoT04VNGSB"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataloader, dataloader_val, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Create directory for the model cleaning it if already exists\n",
    "    makedirs_clean(f\"models/{num_epochs}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        invalid_files = 0\n",
    "\n",
    "        for images, targets in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            # Skip images with incorrect annotations\n",
    "            if torch.equal(targets, torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device)):\n",
    "                invalid_files += 1\n",
    "                continue\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            loss = loss_function(predictions, targets)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            iou = 0.0\n",
    "            for i in range(len(predictions)):\n",
    "                pred_bbox = predictions[i, :4]\n",
    "                target_bbox = targets[i, :4]\n",
    "                # convert values to (xmin, ymin, xmax, ymax)\n",
    "                pred_bbox = (pred_bbox[0] - pred_bbox[2] / 2, pred_bbox[1] - pred_bbox[3] / 2,\n",
    "                            pred_bbox[0] + pred_bbox[2] / 2, pred_bbox[1] + pred_bbox[3] / 2)\n",
    "                target_bbox = (target_bbox[0] - target_bbox[2] / 2, target_bbox[1] - target_bbox[3] / 2,\n",
    "                            target_bbox[0] + target_bbox[2] / 2, target_bbox[1] + target_bbox[3] / 2)\n",
    "\n",
    "                iou += calculate_iou(pred_bbox, target_bbox)\n",
    "\n",
    "            train_acc += iou/len(predictions)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        train_loss = train_loss / (len(dataloader) - invalid_files)\n",
    "        train_acc = train_acc / (len(dataloader) - invalid_files)\n",
    "\n",
    "        # Measure the accuracy using the dataloader_val\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        invalid_files = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(dataloader_val, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "                if torch.equal(targets, torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device)):\n",
    "                    invalid_files += 1\n",
    "                    continue\n",
    "\n",
    "                predictions = model(images)\n",
    "                # Calculate and accumulate accuracy metric across all batches\n",
    "                iou = 0.0\n",
    "                for i in range(len(predictions)):\n",
    "                    pred_bbox = predictions[i, :4]\n",
    "                    target_bbox = targets[i, :4]\n",
    "                    # convert values to (xmin, ymin, xmax, ymax)\n",
    "                    pred_bbox = (pred_bbox[0] - pred_bbox[2] / 2, pred_bbox[1] - pred_bbox[3] / 2,\n",
    "                                pred_bbox[0] + pred_bbox[2] / 2, pred_bbox[1] + pred_bbox[3] / 2)\n",
    "                    target_bbox = (target_bbox[0] - target_bbox[2] / 2, target_bbox[1] - target_bbox[3] / 2,\n",
    "                                target_bbox[0] + target_bbox[2] / 2, target_bbox[1] + target_bbox[3] / 2)\n",
    "\n",
    "                    iou += calculate_iou(pred_bbox, target_bbox)\n",
    "\n",
    "                val_acc += iou/len(predictions)\n",
    "\n",
    "        val_acc = val_acc / (len(dataloader_val) - invalid_files)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the loss and accuracy for each epoch in a txt file\n",
    "        with open(f\"models/{num_epochs}/train_metrics.txt\", \"a\") as f:\n",
    "            f.write(f\"{epoch+1},{train_loss:.6f},{train_acc:.4f},{val_acc:.4f}\\n\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}, Accuracy: {val_acc:.4f}\")\n",
    "        # Stores the model with the best accuracy\n",
    "        if val_acc > best_accuracy:\n",
    "            save_model(model, num_epochs)\n",
    "            best_loss = train_loss\n",
    "            best_accuracy = val_acc\n",
    "            best_epoch = epoch\n",
    "            print(f\"*** Best Epoch #{epoch+1} ***\")\n",
    "\n",
    "\t\t# Early stoping if the accuracy does not improve after 10 epochs\n",
    "        if epoch - best_epoch > 10:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return best_loss, best_accuracy, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqKuF67SQvyZ"
   },
   "source": [
    "## **Usage of the CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnKOsX3TQ2y_"
   },
   "source": [
    "### Set the dataset path and prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdbNRLMaQ0Ot"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"organized_dataset\"\n",
    "\n",
    "image_train_path = f\"{dataset_path}/images/train\"\n",
    "annotations_train_path = f\"{dataset_path}/labels/train\"\n",
    "\n",
    "val_images_path = f\"{dataset_path}/images/val\"\n",
    "val_annotations_path = f\"{dataset_path}/labels/val\"\n",
    "\n",
    "# Prepare Dataset and DataLoader for training\n",
    "dataset = AnodeDataset(image_train_path, annotations_train_path, img_size=256, transform=None)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Prepare Dataset and DataLoader for validation\n",
    "val_dataset = AnodeDataset(val_images_path, val_annotations_path, img_size=256, transform=None)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = NeuralNetwork(img_size=256)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wWbVwzQRFG-"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q0B1woQRLDO"
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "num_epochs = 40\n",
    "_, _, best_epoch = train_model(model, dataloader, val_dataloader, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-qRMx2iRlPn"
   },
   "source": [
    "### Evaluation of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PULBQuJRnrm"
   },
   "outputs": [],
   "source": [
    "def plot_training_data(num_epochs):\n",
    "\n",
    "\t# Get data from the training metrics file\n",
    "\twith open(f\"models/{num_epochs}/train_metrics.txt\", \"r\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\tepochs = []\n",
    "\tlosses = []\n",
    "\taccuracies_train = []\n",
    "\taccuracies_val = []\n",
    "\n",
    "\tfor line in lines:\n",
    "\t\tepoch, loss, acc_train, acc_val = map(float, line.strip().split(\",\"))\n",
    "\t\tepochs.append(epoch)\n",
    "\t\tlosses.append(loss)\n",
    "\t\taccuracies_train.append(acc_train)\n",
    "\t\taccuracies_val.append(acc_val)\n",
    "\n",
    "\tfig, ax1 = plt.subplots()\n",
    "\n",
    "\tcolor = 'tab:red'\n",
    "\tax1.set_xlabel('Epoch')\n",
    "\tax1.set_ylabel('Loss', color=color)\n",
    "\tax1.plot(epochs, losses, color=color)\n",
    "\tax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\tax2 = ax1.twinx()\n",
    "\tcolor = 'tab:blue'\n",
    "\tax2.set_ylabel('Accuracy', color=color)\n",
    "\tax2.plot(epochs, accuracies_train, color=color)\n",
    "\tax2.plot(epochs, accuracies_val, color=\"green\")\n",
    "\tax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\tfig.tight_layout()\n",
    "\tplt.title(f\"Training Metrics for {num_epochs} epochs\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TirJwPrRqVn"
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "plot_training_data(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6pdplH1fJCV"
   },
   "source": [
    "### Evaluation of the model in the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLSd9xSefRvq"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, img_size):\n",
    "    model = NeuralNetwork(img_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to process image and make prediction\n",
    "def predict(image_path, model, img_size):\n",
    "    # Load and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_resized = cv2.resize(img, (img_size, img_size))\n",
    "    img_normalized = img_resized / 255.0  # Normalize to [0, 1]\n",
    "    img_tensor = torch.tensor(img_normalized, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    # Make the prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "\n",
    "    # Convert prediction to bounding box (x_center, y_center, width, height)\n",
    "    x_center, y_center, width, height = prediction[0]\n",
    "    x_center, y_center = x_center.item(), y_center.item()\n",
    "    width, height = width.item(), height.item()\n",
    "\n",
    "    # Return the bounding box and confidence\n",
    "    return x_center, y_center, width, height, img_resized\n",
    "\n",
    "# Function to load ground truth\n",
    "def load_ground_truth(annotation_path, img_size):\n",
    "    if os.path.exists(annotation_path) and os.path.getsize(annotation_path) > 0:\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            line = f.readline().strip()\n",
    "        if line:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "            return x_center, y_center, width, height\n",
    "\n",
    "    return None  # No ground truth\n",
    "\n",
    "\n",
    "# Function to visualize prediction on image\n",
    "def visualize_prediction(pred_box, gt_box, img):\n",
    "    # Draw predicted bounding box if confidence is above threshold\n",
    "    if pred_box is not None:\n",
    "        x1, y1, x2, y2 = pred_box\n",
    "        # Convert to image size\n",
    "        x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Draw ground truth bounding box if available\n",
    "    if gt_box is not None:\n",
    "        x1, y1, x2, y2 = gt_box\n",
    "        # Convert to image size\n",
    "        x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Show the image with both predicted and ground truth boxes\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma5fiThqfZxG"
   },
   "outputs": [],
   "source": [
    "model = load_model(f\"models/{num_epochs}/anode_detector_{num_epochs}.pth\", img_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGsGeSKDfcLV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_images_path = f\"{dataset_path}/images/test\"\n",
    "test_annotations_path = f\"{dataset_path}/labels/test\"\n",
    "\n",
    "test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path)]\n",
    "test_annotations = [os.path.join(test_annotations_path, os.path.splitext(os.path.basename(img))[0] + \".txt\") for img in test_images]\n",
    "\n",
    "# Get average IoU for all test images\n",
    "iou_values = []\n",
    "\n",
    "for image_path, annotation_path in tqdm(zip(test_images, test_annotations), total=len(test_images), desc=\"Processing test images\"):\n",
    "\tx_center, y_center, width, height, img = predict(image_path, model, img_size=256)\n",
    "\t# Load ground truth\n",
    "\tgt_box = load_ground_truth(annotation_path, img_size=256)\n",
    "\t# Calculate IoU if ground truth exists\n",
    "\tif gt_box is not None:\n",
    "\t\t# Convert bounding box to (xmin, ymin, xmax, ymax) format\n",
    "\t\tpred_box = (x_center - width / 2, y_center - height / 2, x_center + width / 2, y_center + height / 2)\n",
    "\t\tgt_box = (gt_box[0] - gt_box[2] / 2, gt_box[1] - gt_box[3] / 2, gt_box[0] + gt_box[2] / 2, gt_box[1] + gt_box[3] / 2)\n",
    "\t\tiou = calculate_iou(pred_box, gt_box)\n",
    "\t\tiou_values.append(iou)\n",
    "\n",
    "print(f\"Average IoU: {np.mean(iou_values):.4f}\")\n",
    "\n",
    "plt.boxplot(iou_values)\n",
    "plt.title(\"IoU Distribution for Test Images\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Displaying 30 random images with predictions and ground truth...\")\n",
    "\n",
    "# Display 30 random images with predictions and ground truth\n",
    "test_images_rand = random.sample(test_images, 30)\n",
    "test_annotations_rand = [os.path.join(test_annotations_path, os.path.splitext(os.path.basename(img))[0] + \".txt\") for img in test_images_rand]\n",
    "\n",
    "# Display the images, predictions and ground truth\n",
    "for image_path, annotation_path in zip(test_images_rand, test_annotations_rand):\n",
    "    # Make prediction\n",
    "    x_center, y_center, width, height, img = predict(image_path, model, img_size=256)\n",
    "    # Convert bounding box to (xmin, ymin, xmax, ymax) format\n",
    "    pred_box = (x_center - width / 2, y_center - height / 2, x_center + width / 2, y_center + height / 2)\n",
    "    # Load ground truth\n",
    "    gt_box = load_ground_truth(annotation_path, img_size=256)\n",
    "\n",
    "    # Calculate IoU if ground truth exists\n",
    "    if gt_box is not None:\n",
    "        # Convert bounding box to (xmin, ymin, xmax, ymax) format\n",
    "        gt_box = (gt_box[0] - gt_box[2] / 2, gt_box[1] - gt_box[3] / 2, gt_box[0] + gt_box[2] / 2, gt_box[1] + gt_box[3] / 2)\n",
    "        iou = calculate_iou(pred_box, gt_box)\n",
    "        print(f\"IoU: {iou:.4f}\")\n",
    "    else:\n",
    "        print(\"No ground truth available for this image.\")\n",
    "\n",
    "    # Visualize the prediction + ground truth (if available)\n",
    "    visualize_prediction(pred_box, gt_box, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WRx3lyyffUp"
   },
   "source": [
    "# **Results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdRxDb91fkdJ"
   },
   "source": [
    "We conducted three evaluations of our model: the first two without data augmentation, and the second with data augmentation. Let’s start by reviewing the results without data augmentation.\n",
    "\n",
    "This model was trained using a batch size of 6 and 15 epochs. The following graph illustrates the evolution of both training and validation accuracy over the epochs, along with the progression of the loss.\n",
    "\n",
    "**add graph**\n",
    "\n",
    "The boxplot below displays the distribution of test accuracy. We can observe some outliers, indicating that the model occasionally struggles with certain cases. However, the spread and central tendency suggest relatively high accuracy levels, indicating that the model performs reasonably well, though not perfectly.\n",
    "\n",
    "**add boxplot**\n",
    "\n",
    "Bellow are some examples of the results obtained with this model.\n",
    "\n",
    "**add images**\n",
    "\n",
    "----------------\n",
    "\n",
    "Our second evaluation was made with the same batch_size as the previous one but with 200 epochs. The evolution of the training and validation accuracy show **...**.\n",
    "\n",
    "**add graph**\n",
    "\n",
    "And the results of the testing dataset got the following boxplot that show **...**\n",
    "\n",
    "**add boxplot**\n",
    "\n",
    "Bellow are some examples of the results obtained with this model.\n",
    "\n",
    "**add images**\n",
    "\n",
    "----------------\n",
    "\n",
    "Our last model was made with the same batch_size and 50 epochs. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gIInp4YlNwLZ",
    "kLj76vrCOAEt",
    "k-WNFMi3QDjJ",
    "OxuSR7txQY1c",
    "LmqWqoTRLucd",
    "Vr1XnxS6MMoO",
    "67n44L1kMYcx"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
